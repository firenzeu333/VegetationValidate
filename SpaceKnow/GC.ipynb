{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating with SpaceKnow GoogleCloud Bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to integrate with an Google Cloud Bucker, more specifically the one used by SpaceKnow(The code is written for their data structure) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code assumes you have the right premission file in your computer (GC certificates) supplied by SpaceKnow - if not you won't be able to access the bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before you start you will have to pip install GC pluging via:\n",
    "pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import required files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "import pymongo\n",
    "import analona\n",
    "from dateutil import parser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can try accessing the bucket - \n",
    "firstly we need to set up all our credentials and specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_name = '<add_bucket_name_here>'\n",
    "credentials_file = '<add_path_to_credentials_here>.json'\n",
    "project_name = '<add_storage_name_here>'\n",
    "scope_url = 'https://www.googleapis.com/auth/devstorage.full_control'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initiate a client with our credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    credentials_file)\n",
    "\n",
    "scoped_credentials = credentials.with_scopes(\n",
    "    [scope_url])\n",
    "\n",
    "storage_client = storage.Client(project_name, scoped_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing so we can connect to our bucket and to our two blob feeds - one for weekly products and one for daily products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "weekly_blobs_list = list(bucket.list_blobs(prefix=\"weekly\"))\n",
    "daily_blobs_list = blobs=list(bucket.list_blobs(prefix=\"daily\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now we can go into parsing all the available products into our MongoDB database - \n",
    "we first have to set up the connection with our mongo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the uri based on the one given in our Azure cloud \n",
    "mongo_uri = ''\n",
    "client = pymongo.MongoClient(mongo_uri)\n",
    "db = client.louvre\n",
    "ships_collection = db.ships\n",
    "airplanes_collection = db.airplanes\n",
    "buildings_collection = db.buildings\n",
    "roads_collection = db.roads\n",
    "vegetation_collection = db.vegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we can go into getting the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_weekly_to_mongo(wrunc_file, aoi, tile, week_string, date_string, record_file):\n",
    "    features = wrunc_file[\"features\"]\n",
    "    index = 1\n",
    "        \n",
    "    date_timestamp, _ = date_string.split('_')\n",
    "    features_date = parser.parse(date_timestamp)\n",
    "    \n",
    "    original_images = [record_file['image']['sceneId']]\n",
    "    \n",
    "    for feature in features:\n",
    "        item_class = feature['properties']['class']\n",
    "        item = {}\n",
    "        item['_id'] = \"SpaceKnow_{}_{}_{}_{}\".format(item_class, tile, week_string, index)\n",
    "        index += 1\n",
    "        item['company'] = \"SpaceKnow\"\n",
    "        item['geometry'] = feature['geometry']\n",
    "        item['analyticsDeliveryTime'] = datetime.utcfromtimestamp(int(round(features_date.timestamp())))\n",
    "        item['analyticsInfo'] = {\n",
    "            'storage': \"GoogleCloud\", \n",
    "            'url': \"gs://{}/weekly/{}/{}/{}/{}\".format(bucket_name, aoi, tile, week_string, date_string)\n",
    "        }\n",
    "        item['tileId'] = record_file['tilePosition'] \n",
    "        print(item)\n",
    "        item['sourceImagesIds'] = original_images\n",
    "        \n",
    "        if item_class == 'roads':\n",
    "            is_valid = analona.Road(item).validate()\n",
    "            if is_valid == True:\n",
    "                roads_collection.update({ \"_id\": item[\"_id\"] }, item, upsert = True)\n",
    "            else:\n",
    "                print(\"didn't pass varification- {}\".format(is_valid))\n",
    "        elif item_class == 'urban':\n",
    "            is_valid = analona.Building(item).validate()            \n",
    "            if is_valid == True:\n",
    "                buildings_collection.update({ \"_id\": item[\"_id\"] }, item, upsert = True)\n",
    "            else:\n",
    "                print(\"didn't pass varification- {}\".format(is_valid))\n",
    "        else:\n",
    "            is_valid = analona.Building(item).validate() \n",
    "            if is_valid == True:\n",
    "                vegetation_collection.update({ \"_id\": item[\"_id\"] }, item, upsert = True)\n",
    "            else:\n",
    "                print(\"didn't pass varification- {}\".format(is_valid))\n",
    "            \n",
    "\n",
    "for blob in weekly_blobs_list: \n",
    "    print(blob.name)\n",
    "    if(blob.name.endswith(\"wrunc_detections.geojson\")):\n",
    "        print(blob.name)\n",
    "        prod_type, aoi, tile, week_string, date_string, file_type = blob.name.split('/')\n",
    "        record_file = \"{}/{}/{}/{}/{}/record.json\".format(prod_type, aoi, tile, week_string, date_string)\n",
    "        record_blob = bucket.blob(record_file)\n",
    "        record_file = json.loads(record_blob.download_as_string())\n",
    "        wrunc_file = json.loads(blob.download_as_string())\n",
    "        parse_weekly_to_mongo(wrunc_file, aoi, tile, week_string, date_string, record_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_blob = bucket.blob(\"weekly/aoi-1/r00c06/2018W45/20181115T072837_LANc/imagery_truecolor.geotiff\")\n",
    "record_blob.download_to_filename(\"image.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we would like to go over the daily products and parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_daily_to_mongo(detection_file, aoi, tile, day_date_string, specific_date_string, record_file):\n",
    "    features = detection_file[\"features\"]\n",
    "    index = 1\n",
    "        \n",
    "    date_timestamp, _ = specific_date_string.split('_')\n",
    "    features_date = parser.parse(date_timestamp)\n",
    "    \n",
    "    original_image = record_file['image']['sceneId']\n",
    "    \n",
    "    for feature in features:\n",
    "        print(feature)\n",
    "        item_class = feature['properties']['class']\n",
    "        item = {}\n",
    "        item['_id'] = \"SpaceKnow_{}_{}_{}_{}\".format(item_class, tile, day_date_string, index)\n",
    "        print(item['_id'])\n",
    "        index += 1\n",
    "        item['company'] = \"SpaceKnow\"\n",
    "        item['geometry'] = feature['geometry']\n",
    "        item['originalImageId'] = original_image\n",
    "        item['observed'] = features_date\n",
    "        item['area'] = feature['properties']['area']\n",
    "        item['direction'] = feature['properties']['orientation']\n",
    "        \n",
    "        if item_class == \"ships\":\n",
    "            is_valid = analona.Ship(item).validate()            \n",
    "            if is_valid == True:\n",
    "                ships_collection.update({ \"_id\": item[\"_id\"] }, item, upsert = True)\n",
    "            else:\n",
    "                print(\"didn't pass varification- {}\".format(is_valid))\n",
    "        elif item_class == \"airplanes\":\n",
    "            is_valid = analona.Plane(item).validate()            \n",
    "            if is_valid == True:\n",
    "                airplanes_collection.update({ \"_id\": item[\"_id\"] }, item, upsert = True)\n",
    "            else:\n",
    "                print(\"didn't pass varification- {}\".format(is_valid))\n",
    "        else: \n",
    "            print(\"unknown object type: {}\".format(item_class))\n",
    "\n",
    "for blob in daily_blobs_list:\n",
    "    print(blob.name)\n",
    "    if(blob.name.endswith(\".geojson\")and not(\"grid\" in blob.name)):\n",
    "        file_type, aoi, tile, day_date, file_timestamp, file_name = blob.name.split('/')\n",
    "        record_file = \"{}/{}/{}/{}/{}/record.json\".format(file_type, aoi, tile, day_date, file_timestamp)\n",
    "        record_blob = bucket.blob(record_file)\n",
    "        record_file = json.loads(record_blob.download_as_string())\n",
    "        detection_file = json.loads(blob.download_as_string())\n",
    "        parse_daily_to_mongo(detection_file, aoi, tile, day_date, file_timestamp, record_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_blob = bucket.blob(\"daily/1-coastline/r00c03/20181120/20181120T075358_5zgP/imagery_truecolor.geotiff\")\n",
    "record_blob.download_to_filename(\"shipImage.tiff\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
